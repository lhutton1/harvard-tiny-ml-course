{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Model - Horse vs Human detector\n",
    "\n",
    "This model will be constructed and trained to detect images of horses or humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-18 13:35:16--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.169.80, 172.217.16.240, 142.250.179.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.169.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 149574867 (143M) [application/zip]\n",
      "Saving to: ‘/tmp/horse-or-human.zip’\n",
      "\n",
      "/tmp/horse-or-human 100%[===================>] 142.65M  12.3MB/s    in 11s     \n",
      "\n",
      "2021-06-18 13:35:28 (12.6 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
      "\n",
      "--2021-06-18 13:35:28--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.169.80, 172.217.16.240, 142.250.179.240, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.169.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11480187 (11M) [application/zip]\n",
      "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
      "\n",
      "/tmp/validation-hor 100%[===================>]  10.95M  12.2MB/s    in 0.9s    \n",
      "\n",
      "2021-06-18 13:35:30 (12.2 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
    "    -O /tmp/horse-or-human.zip\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
    "    -O /tmp/validation-horse-or-human.zip\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "zip_files = ['/tmp/horse-or-human.zip', '/tmp/validation-horse-or-human.zip']\n",
    "zip_ref = None\n",
    "for zip_file in zip_files:\n",
    "    zip_ref = zipfile.ZipFile(zip_file, 'r')\n",
    "    zip_ref.extractall(zip_file[:-4])\n",
    "if zip_ref:\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 47, 47, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 6,778,945\n",
      "Trainable params: 6,778,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=1e-3),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise data into generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# We can generate more traning data by augmenting our images using different techniques. This can help improve accuracy when there is little data available, however it can also lead to overfitting.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    #rotation_range=40,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    #horizontal_flip=True,\n",
    "    #fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/tmp/horse-or-human',\n",
    "    target_size=(100, 100),\n",
    "    batch_size=128,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    '/tmp/validation-horse-or-human',\n",
    "    target_size=(100, 100),\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.5628 - acc: 0.5106 - val_loss: 0.6631 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6694 - acc: 0.6340 - val_loss: 0.6532 - val_acc: 0.5117\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 16s 2s/step - loss: 0.6765 - acc: 0.7186 - val_loss: 1.2128 - val_acc: 0.6680\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 18s 2s/step - loss: 0.3910 - acc: 0.8521 - val_loss: 0.3993 - val_acc: 0.8633\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3455 - acc: 0.8932 - val_loss: 1.0156 - val_acc: 0.8125\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1744 - acc: 0.9444 - val_loss: 0.8880 - val_acc: 0.8359\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0726 - acc: 0.9800 - val_loss: 1.4072 - val_acc: 0.8125\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4337 - acc: 0.8877 - val_loss: 0.4474 - val_acc: 0.8242\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.2114 - acc: 0.9299 - val_loss: 1.0066 - val_acc: 0.8438\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0341 - acc: 0.9902 - val_loss: 0.9036 - val_acc: 0.8789\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.2064 - acc: 0.9375 - val_loss: 0.3999 - val_acc: 0.9023\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1777 - acc: 0.9255 - val_loss: 1.3118 - val_acc: 0.8203\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0164 - acc: 0.9951 - val_loss: 1.1184 - val_acc: 0.8750\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0064 - acc: 0.9978 - val_loss: 1.3316 - val_acc: 0.8633\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.7705 - val_acc: 0.8516\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.2969 - acc: 0.9177 - val_loss: 2.9656 - val_acc: 0.6602\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0314 - acc: 0.9902 - val_loss: 1.2975 - val_acc: 0.8594\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0089 - acc: 0.9989 - val_loss: 1.3917 - val_acc: 0.8555\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.0069 - acc: 0.9978 - val_loss: 2.6060 - val_acc: 0.7930\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0011 - acc: 1.0000 - val_loss: 3.1753 - val_acc: 0.7656\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.1348e-04 - acc: 1.0000 - val_loss: 2.7071 - val_acc: 0.8281\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 6.7660e-05 - acc: 1.0000 - val_loss: 2.9589 - val_acc: 0.8281\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 4.1787e-05 - acc: 1.0000 - val_loss: 3.1103 - val_acc: 0.8281\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.7515e-05 - acc: 1.0000 - val_loss: 3.2506 - val_acc: 0.8281\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.3511e-05 - acc: 1.0000 - val_loss: 3.3263 - val_acc: 0.8281\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 9.9514e-06 - acc: 1.0000 - val_loss: 3.4559 - val_acc: 0.8281\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 6.7291e-06 - acc: 1.0000 - val_loss: 3.5578 - val_acc: 0.8320\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.7261e-06 - acc: 1.0000 - val_loss: 3.6761 - val_acc: 0.8320\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.5685e-06 - acc: 1.0000 - val_loss: 3.7241 - val_acc: 0.8359\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.2621e-06 - acc: 1.0000 - val_loss: 3.9150 - val_acc: 0.8359\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0043e-06 - acc: 1.0000 - val_loss: 3.9311 - val_acc: 0.8398\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 7.0693e-07 - acc: 1.0000 - val_loss: 3.8816 - val_acc: 0.8438\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 4.4222e-07 - acc: 1.0000 - val_loss: 4.1595 - val_acc: 0.8398\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.8981e-07 - acc: 1.0000 - val_loss: 4.1108 - val_acc: 0.8438\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.4838e-07 - acc: 1.0000 - val_loss: 4.1347 - val_acc: 0.8438\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.7898e-07 - acc: 1.0000 - val_loss: 4.1139 - val_acc: 0.8477\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 7.4088e-08 - acc: 1.0000 - val_loss: 4.2868 - val_acc: 0.8477\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 5.0696e-08 - acc: 1.0000 - val_loss: 4.2972 - val_acc: 0.8477\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 13s 1s/step - loss: 4.0550e-08 - acc: 1.0000 - val_loss: 4.5472 - val_acc: 0.8438\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.3119e-08 - acc: 1.0000 - val_loss: 4.5807 - val_acc: 0.8477\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 4.6933 - acc: 0.8409 - val_loss: 0.7415 - val_acc: 0.8633\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1458 - acc: 0.9566 - val_loss: 1.3042 - val_acc: 0.7852\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0160 - acc: 0.9967 - val_loss: 1.3180 - val_acc: 0.8242\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0050 - acc: 0.9989 - val_loss: 1.5530 - val_acc: 0.8438\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.7564 - val_acc: 0.8398\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.7265e-04 - acc: 1.0000 - val_loss: 1.9896 - val_acc: 0.8438\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.3020e-04 - acc: 1.0000 - val_loss: 2.0629 - val_acc: 0.8438\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 6.8315e-05 - acc: 1.0000 - val_loss: 2.1091 - val_acc: 0.8477\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 3.8775e-05 - acc: 1.0000 - val_loss: 2.2527 - val_acc: 0.8438\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 3.4173e-05 - acc: 1.0000 - val_loss: 2.3251 - val_acc: 0.8477\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.4894e-05 - acc: 1.0000 - val_loss: 2.4043 - val_acc: 0.8516\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4064 - acc: 0.9566 - val_loss: 0.3139 - val_acc: 0.9258\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0603 - acc: 0.9822 - val_loss: 1.4414 - val_acc: 0.8516\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0012 - acc: 1.0000 - val_loss: 2.0651 - val_acc: 0.8398\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.8942e-04 - acc: 1.0000 - val_loss: 2.1242 - val_acc: 0.8516\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.1242e-04 - acc: 1.0000 - val_loss: 2.2375 - val_acc: 0.8633\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 4.5558e-05 - acc: 1.0000 - val_loss: 2.3901 - val_acc: 0.8672\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.1621e-05 - acc: 1.0000 - val_loss: 2.5528 - val_acc: 0.8633\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.8945e-05 - acc: 1.0000 - val_loss: 2.6711 - val_acc: 0.8672\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 8.1921e-06 - acc: 1.0000 - val_loss: 2.8132 - val_acc: 0.8672\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 5.2419e-06 - acc: 1.0000 - val_loss: 2.7569 - val_acc: 0.8711\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.8679e-06 - acc: 1.0000 - val_loss: 2.9800 - val_acc: 0.8672\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 13s 2s/step - loss: 3.0840e-06 - acc: 1.0000 - val_loss: 3.2247 - val_acc: 0.8633\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.0844e-06 - acc: 1.0000 - val_loss: 3.3394 - val_acc: 0.8711\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 7.1447e-07 - acc: 1.0000 - val_loss: 3.4138 - val_acc: 0.8672\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 5.9979e-07 - acc: 1.0000 - val_loss: 3.4241 - val_acc: 0.8711\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.1245 - acc: 0.9622 - val_loss: 22.6209 - val_acc: 0.5820\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6258 - acc: 0.9455 - val_loss: 1.1024 - val_acc: 0.8477\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.0109 - acc: 0.9978 - val_loss: 1.1940 - val_acc: 0.8594\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0036 - acc: 0.9990 - val_loss: 1.4450 - val_acc: 0.8438\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 6.0898e-04 - acc: 1.0000 - val_loss: 1.4469 - val_acc: 0.8672\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.1174e-04 - acc: 1.0000 - val_loss: 1.5977 - val_acc: 0.8672\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 8.7223e-05 - acc: 1.0000 - val_loss: 1.7960 - val_acc: 0.8633\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 4.7091e-05 - acc: 1.0000 - val_loss: 1.9349 - val_acc: 0.8594\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 2.8585e-05 - acc: 1.0000 - val_loss: 2.1197 - val_acc: 0.8516\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.8003e-05 - acc: 1.0000 - val_loss: 2.2266 - val_acc: 0.8516\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0782e-05 - acc: 1.0000 - val_loss: 2.2508 - val_acc: 0.8633\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 7.0070e-06 - acc: 1.0000 - val_loss: 2.3796 - val_acc: 0.8594\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 6.0378e-06 - acc: 1.0000 - val_loss: 2.5485 - val_acc: 0.8555\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.9933e-06 - acc: 1.0000 - val_loss: 2.6150 - val_acc: 0.8594\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.0116e-06 - acc: 1.0000 - val_loss: 2.7665 - val_acc: 0.8555\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.7720e-06 - acc: 1.0000 - val_loss: 2.7629 - val_acc: 0.8594\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 4.9816e-06 - acc: 1.0000 - val_loss: 3.3655 - val_acc: 0.8242\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.5013e-06 - acc: 1.0000 - val_loss: 3.1321 - val_acc: 0.8516\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 8.6130e-07 - acc: 1.0000 - val_loss: 3.2515 - val_acc: 0.8516\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 2.6685e-07 - acc: 1.0000 - val_loss: 3.2943 - val_acc: 0.8516\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.9698e-07 - acc: 1.0000 - val_loss: 3.3952 - val_acc: 0.8516\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.1815e-07 - acc: 1.0000 - val_loss: 3.4817 - val_acc: 0.8477\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 9.8391e-08 - acc: 1.0000 - val_loss: 3.5586 - val_acc: 0.8438\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 7.0932e-08 - acc: 1.0000 - val_loss: 3.6050 - val_acc: 0.8477\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 5.6792e-08 - acc: 1.0000 - val_loss: 3.6794 - val_acc: 0.8477\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.2465e-08 - acc: 1.0000 - val_loss: 3.7512 - val_acc: 0.8477\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 2.4183e-08 - acc: 1.0000 - val_loss: 3.9544 - val_acc: 0.8398\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.3214e-08 - acc: 1.0000 - val_loss: 3.9699 - val_acc: 0.8438\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.3035e-08 - acc: 1.0000 - val_loss: 4.0819 - val_acc: 0.8438\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 7.7297e-09 - acc: 1.0000 - val_loss: 4.1739 - val_acc: 0.8398\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 7.5719e-09 - acc: 1.0000 - val_loss: 4.1628 - val_acc: 0.8477\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 5.6598e-09 - acc: 1.0000 - val_loss: 4.2880 - val_acc: 0.8438\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 3.6441e-09 - acc: 1.0000 - val_loss: 4.3720 - val_acc: 0.8438\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 2.7459e-09 - acc: 1.0000 - val_loss: 4.4748 - val_acc: 0.8438\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "Run the model by uploading an image of your choice. The output will be a prediction - a horse or a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca554124271b430cad819b1fcf7c3802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import FileUpload\n",
    "upload = FileUpload()\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4459301e-20]]\n",
      "[1.4459301e-20]\n",
      "download.webp is a horse\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "for name, data in upload.value.items():\n",
    "    raw_data = data['content']\n",
    "    img = Image.open(io.BytesIO(raw_data))\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize((100, 100), Image.NEAREST)\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = x / 255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    image_tensor = np.vstack([x])\n",
    "    classes = model.predict(image_tensor)\n",
    "    if classes[0]>0.5:\n",
    "        print(name + \" is a human\")\n",
    "    else:\n",
    "        print(name + \" is a horse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
